---
source_url: https://arxiv.org/html/2505.22101v1
paper_id: 2505.22101v1
title: MemOS: An Operating System for Memory-Augmented Generation (MAG) in Large Language Models (Short Version)
scraped_date: 2025-10-16
has_images: yes
has_tables: yes
---

## References

- Brown et al. [2020]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.Language Models are Few-Shot Learners, July 2020.URLhttp://arxiv.org/abs/2005.14165.arXiv:2005.14165 [cs].
- Cao et al. [2021]Nicola De Cao, Wilker Aziz, and Ivan Titov.Editing Factual Knowledge in Language Models, September 2021.URLhttp://arxiv.org/abs/2104.08164.arXiv:2104.08164 [cs].
- Chen et al. [2023]Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz.Walking down the memory maze: Beyond context limit through interactive reading.CoRR, abs/2310.05029, 2023.10.48550/ARXIV.2310.05029.URLhttps://doi.org/10.48550/arXiv.2310.05029.
- Chhikara et al. [2025]Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav.Mem0: Building production-ready ai agents with scalable long-term memory, 2025.URLhttps://arxiv.org/abs/2504.19413.
- Devlin et al. [2019]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.In Jill Burstein, Christy Doran, and Thamar Solorio, editors,Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.10.18653/v1/N19-1423.URLhttps://aclanthology.org/N19-1423/.
- Dong et al. [2024]Harry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie Chi, and Beidi Chen.Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference.June 2024.URLhttps://openreview.net/forum?id=uhHDhVKFMW.
- Du et al. [2025]Yiming Du, Wenyu Huang, Danna Zheng, Zhaowei Wang, Sebastien Montella, Mirella Lapata, Kam-Fai Wong, and Jeff Z. Pan.Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions, May 2025.URLhttp://arxiv.org/abs/2505.00675.arXiv:2505.00675 [cs].
- Edge et al. [2024]Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson.From local to global: A graph RAG approach to query-focused summarization.CoRR, abs/2404.16130, 2024.10.48550/ARXIV.2404.16130.URLhttps://doi.org/10.48550/arXiv.2404.16130.
- Fang et al. [2025]Junfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Shi Jie, Xiang Wang, Xiangnan He, and Tat-seng Chua.AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models, March 2025.URLhttp://arxiv.org/abs/2410.02355.arXiv:2410.02355 [cs].
- Gao et al. [2023]Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang.Retrieval-augmented generation for large language models: A survey.CoRR, abs/2312.10997, 2023.10.48550/ARXIV.2312.10997.URLhttps://doi.org/10.48550/arXiv.2312.10997.
- Guo et al. [2024]Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang.Lightrag: Simple and fast retrieval-augmented generation.CoRR, abs/2410.05779, 2024.10.48550/ARXIV.2410.05779.URLhttps://doi.org/10.48550/arXiv.2410.05779.
- Gutierrez et al. [2024]Bernal Jimenez Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su.Hipporag: Neurobiologically inspired long-term memory for large language models.In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors,Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024.URLhttp://papers.nips.cc/paper_files/paper/2024/hash/6ddc001d07ca4f319af96a3024f6dbd1-Abstract-Conference.html.
- Hsieh et al. [2023]Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes.In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors,Findings of the Association for Computational Linguistics: ACL 2023, pages 8003–8017, Toronto, Canada, July 2023. Association for Computational Linguistics.10.18653/v1/2023.findings-acl.507.URLhttps://aclanthology.org/2023.findings-acl.507/.
- Hu et al. [2021]Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.LoRA: Low-Rank Adaptation of Large Language Models, October 2021.URLhttp://arxiv.org/abs/2106.09685.arXiv:2106.09685 [cs].
- Khandelwal et al. [2019]Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis.Generalization through Memorization: Nearest Neighbor Language Models.September 2019.URLhttps://openreview.net/forum?id=HklBjCEKvH.
- Kwon et al. [2023]Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica.Efficient Memory Management for Large Language Model Serving with PagedAttention.InProceedings of the 29th Symposium on Operating Systems Principles, pages 611–626, Koblenz Germany, October 2023. ACM.ISBN 979-8-4007-0229-7.10.1145/3600006.3613165.URLhttps://dl.acm.org/doi/10.1145/3600006.3613165.
- Liang et al. [2024]Xiang Liang, Simin Niu, Zhiyu Li, Sensen Zhang, Shichao Song, Hanyu Wang, Jiawei Yang, Feiyu Xiong, Bo Tang, and Chenyang Xi.Empowering large language models to set up a knowledge retrieval indexer via self-learning.CoRR, abs/2405.16933, 2024.10.48550/ARXIV.2405.16933.URLhttps://doi.org/10.48550/arXiv.2405.16933.
- Liu et al. [2024]Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.Lost in the Middle: How Language Models Use Long Contexts.Transactions of the Association for Computational Linguistics, 12:157–173, 2024.10.1162/tacl_a_00638.URLhttps://aclanthology.org/2024.tacl-1.9/.Place: Cambridge, MA Publisher: MIT Press.
- Meng et al. [2023a]Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.Locating and Editing Factual Associations in GPT, January 2023a.URLhttp://arxiv.org/abs/2202.05262.arXiv:2202.05262 [cs].
- Meng et al. [2023b]Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau.Mass-Editing Memory in a Transformer, August 2023b.URLhttp://arxiv.org/abs/2210.07229.arXiv:2210.07229 [cs].
- Ouyang et al. [2022]Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.Training language models to follow instructions with human feedback, March 2022.URLhttp://arxiv.org/abs/2203.02155.arXiv:2203.02155 [cs].
- Packer et al. [2024]Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez.MemGPT: Towards LLMs as Operating Systems, February 2024.URLhttp://arxiv.org/abs/2310.08560.arXiv:2310.08560 [cs].
- Shan et al. [2025]Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, and Yong Wu.Cognitive Memory in Large Language Models, April 2025.URLhttp://arxiv.org/abs/2504.02441.arXiv:2504.02441 [cs].
- Su et al. [2025]Weihang Su, Yichen Tang, Qingyao Ai, Junxi Yan, Changyue Wang, Hongning Wang, Ziyi Ye, Yujia Zhou, and Yiqun Liu.Parametric Retrieval Augmented Generation, January 2025.URLhttp://arxiv.org/abs/2501.15915.arXiv:2501.15915 [cs].
- Subramani et al. [2022]Nishant Subramani, Nivedita Suresh, and Matthew Peters.Extracting Latent Steering Vectors from Pretrained Language Models.InFindings of the Association for Computational Linguistics: ACL 2022, pages 566–581, Dublin, Ireland, 2022. Association for Computational Linguistics.10.18653/v1/2022.findings-acl.48.URLhttps://aclanthology.org/2022.findings-acl.48.
- Tan et al. [2025]Yuqiao Tan, Shizhu He, Huanxuan Liao, Jun Zhao, and Kang Liu.Better wit than wealth: Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement, March 2025.URLhttp://arxiv.org/abs/2503.23895.arXiv:2503.23895 [cs].
- Turner et al. [2024]Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan J. Vazquez, Ulisse Mini, and Monte MacDiarmid.Steering Language Models With Activation Engineering, October 2024.URLhttp://arxiv.org/abs/2308.10248.arXiv:2308.10248 [cs].
- Wang et al. [2025]Tianlong Wang, Xianfeng Jiao, Yinghao Zhu, Zhongzhi Chen, Yifan He, Xu Chu, Junyi Gao, Yasha Wang, and Liantao Ma.Adaptive Activation Steering: A Tuning-Free LLM Truthfulness Improvement Method for Diverse Hallucinations Categories.InProceedings of the ACM on Web Conference 2025, WWW ’25, pages 2562–2578, New York, NY, USA, April 2025. Association for Computing Machinery.ISBN 979-8-4007-1274-6.10.1145/3696410.3714640.URLhttps://dl.acm.org/doi/10.1145/3696410.3714640.
- Wei et al. [2025]Jiale Wei, Xiang Ying, Tao Gao, Fangyi Bao, Felix Tao, and Jingbo Shang.Ai-native memory 2.0: Second me, 2025.URLhttps://arxiv.org/abs/2503.08102.
- Wu et al. [2025]Yaxiong Wu, Sheng Liang, Chen Zhang, Yichao Wang, Yongyue Zhang, Huifeng Guo, Ruiming Tang, and Yong Liu.From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs, April 2025.URLhttp://arxiv.org/abs/2504.15965.arXiv:2504.15965 [cs].
- Xu et al. [2025a]Tianyang Xu, Haojie Zheng, Chengze Li, Haoxiang Chen, Yixin Liu, Ruoxi Chen, and Lichao Sun.Noderag: Structuring graph-based rag with heterogeneous nodes, 2025a.URLhttps://arxiv.org/abs/2504.11544.
- Xu et al. [2025b]Xin Xu, Wei Xu, Ningyu Zhang, and Julian McAuley.BiasEdit: Debiasing Stereotyped Language Models via Model Editing, March 2025b.URLhttp://arxiv.org/abs/2503.08588.arXiv:2503.08588 [cs].
- Xu et al. [2025c]Ziwen Xu, Shuxun Wang, Kewei Xu, Haoming Xu, Mengru Wang, Xinle Deng, Yunzhi Yao, Guozhou Zheng, Huajun Chen, and Ningyu Zhang.EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models, April 2025c.URLhttp://arxiv.org/abs/2504.15133.arXiv:2504.15133 [cs].
- Yang et al. [2024]Hongkang Yang, Zehao Lin, Wenjin Wang, Hao Wu, Zhiyu Li, Bo Tang, Wenqiang Wei, Jinbo Wang, Zeyun Tang, Shichao Song, Chenyang Xi, Yu Yu, Kai Chen, Feiyu Xiong, Linpeng Tang, and Weinan E.$\text{Memory}^3$: Language Modeling with Explicit Memory.Journal of Machine Learning, 3(3):300–346, January 2024.ISSN 2790-2048, 2790-203X.10.4208/jml.240708.URLhttp://arxiv.org/abs/2407.01178.arXiv:2407.01178 [cs].
- Yang et al. [2025]Peiru Yang, Xintian Li, Zhiyang Hu, Jiapeng Wang, Jinhua Yin, Huili Wang, Lizhi He, Shuai Yang, Shangguang Wang, Yongfeng Huang, and Tao Qi.Heterag: A heterogeneous retrieval-augmented generation framework with decoupled knowledge representations, 2025.URLhttps://arxiv.org/abs/2504.10529.
- Zhang et al. [2024]Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, and Huajun Chen.A Comprehensive Study of Knowledge Editing for Large Language Models, November 2024.URLhttp://arxiv.org/abs/2401.01286.arXiv:2401.01286 [cs].
- Zhao et al. [2023a]Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and Mengnan Du.Explainability for large language models: A survey, 2023a.URLhttps://arxiv.org/abs/2309.01029.
- Zhao et al. [2024]Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, and Bin Cui.Retrieval-augmented generation for ai-generated content: A survey.CoRR, abs/2402.19473, 2024.10.48550/ARXIV.2402.19473.URLhttps://doi.org/10.48550/arXiv.2402.19473.
- Zhao et al. [2023b]Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.A survey of large language models.arXiv preprint arXiv:2303.18223, 1(2), 2023b.