---
source_url: https://arxiv.org/html/2507.03724v3
paper_id: 2507.03724v3
title: \titlefontMemOS: A Memory OS for AI System
scraped_date: 2025-10-16
has_images: yes
has_tables: yes
---

## References

- [1]Hongkang Yang, Zehao Lin, Wenjin Wang, Hao Wu, Zhiyu Li, Bo Tang, Wenqiang Wei, Jinbo Wang, Zeyun Tang, Shichao Song, Chenyang Xi, Yu Yu, Kai Chen, Feiyu Xiong, Linpeng Tang, and Weinan E.Memory3: Language modeling with explicit memory.Journal of Machine Learning, 3(3):300–346, January 2024.
- [2]Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.A survey of large language models.arXiv preprint arXiv:2303.18223, 1(2), 2023.
- [3]Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi.Codet5+: Open code large language models for code understanding and generation.arXiv preprint arXiv:2305.07922, 2023.
- [4]Shengsheng Qian, Zuyi Zhou, Dizhan Xue, Bing Wang, and Changsheng Xu.From linguistic giants to sensory maestros: A survey on cross-modal reasoning with large language models.arXiv preprint arXiv:2409.18996, 2024.
- [5]Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, and Bin Cui.Retrieval-augmented generation for ai-generated content: A survey.CoRR, abs/2402.19473, 2024.
- [6]Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang.Retrieval-augmented generation for large language models: A survey.CoRR, abs/2312.10997, 2023.
- [7]Qinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou, Zijin Hong, Junnan Dong, Hao Chen, Yi Chang, and Xiao Huang.A survey of graph retrieval-augmented generation for customized large language models.CoRR, abs/2501.13958, 2025.
- [8]Bo Ni, Zheyuan Liu, Leyao Wang, Yongjia Lei, Yuying Zhao, Xueqi Cheng, Qingkai Zeng, Luna Dong, Yinglong Xia, Krishnaram Kenthapadi, Ryan A. Rossi, Franck Dernoncourt, Md. Mehrab Tanjim, Nesreen K. Ahmed, Xiaorui Liu, Wenqi Fan, Erik Blasch, Yu Wang, Meng Jiang, and Tyler Derr.Towards trustworthy retrieval augmented generation for large language models: A survey.CoRR, abs/2502.06872, 2025.
- [9]Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz.Walking down the memory maze: Beyond context limit through interactive reading.CoRR, abs/2310.05029, 2023.
- [10]Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson.From local to global: A graph RAG approach to query-focused summarization.CoRR, abs/2404.16130, 2024.
- [11]Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang.Lightrag: Simple and fast retrieval-augmented generation.CoRR, abs/2410.05779, 2024.
- [12]Microsoft.Retrieval augmented generation (rag) in azure ai search, 2025.
- [13]Google.Vertex ai search, 2025.
- [14]Elastic.Build innovative ai search experiences, 2025.
- [15]Nuclia.Agentic rag-as-a-service company, 2025.
- [16]Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, and Thang Luong.Freshllms: Refreshing large language models with search engine augmentation, 2023.
- [17]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.Gpt-4 technical report.arXiv preprint arXiv:2303.08774, 2023.
- [18]Cursor - The AI Code Editor.
- [19]Yiming Du, Wenyu Huang, Danna Zheng, Zhaowei Wang, Sebastien Montella, Mirella Lapata, Kam-Fai Wong, and Jeff Z. Pan.Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions, May 2025.arXiv:2505.00675 [cs].
- [20]Yaxiong Wu, Sheng Liang, Chen Zhang, Yichao Wang, Yongyue Zhang, Huifeng Guo, Ruiming Tang, and Yong Liu.From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs, April 2025.arXiv:2504.15965 [cs].
- [21]Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, and Yong Wu.Cognitive Memory in Large Language Models, April 2025.arXiv:2504.02441 [cs].
- [22]Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.Language Models are Unsupervised Multitask Learners.
- [23]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.Language Models are Few-Shot Learners, July 2020.arXiv:2005.14165 [cs].
- [24]Xiang Lisa Li and Percy Liang.Prefix-Tuning: Optimizing Continuous Prompts for Generation.InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597, Online, 2021. Association for Computational Linguistics.
- [25]Brian Lester, Rami Al-Rfou, and Noah Constant.The Power of Scale for Parameter-Efficient Prompt Tuning.InProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045–3059, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics.
- [26]Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang.GPT Understands, Too, October 2023.arXiv:2103.10385 [cs].
- [27]Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang.P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks.InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 61–68, Dublin, Ireland, 2022. Association for Computational Linguistics.
- [28]Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.Training language models to follow instructions with human feedback, March 2022.arXiv:2203.02155 [cs].
- [29]Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica.Efficient Memory Management for Large Language Model Serving with PagedAttention.InProceedings of the 29th Symposium on Operating Systems Principles, pages 611–626, Koblenz Germany, October 2023. ACM.
- [30]Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis.Efficient Streaming Language Models with Attention Sinks.October 2023.
- [31]Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, Zhangyang "Atlas" Wang, and Beidi Chen.H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models.Advances in Neural Information Processing Systems, 36:34661–34710, December 2023.
- [32]Harry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie Chi, and Beidi Chen.Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference.June 2024.
- [33]Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney, Yakun S. Shao, Kurt Keutzer, and Amir Gholami.KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization.Advances in Neural Information Processing Systems, 37:1270–1303, December 2024.
- [34]Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, and Lili Qiu.RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval, December 2024.arXiv:2409.10516 [cs].
- [35]Nishant Subramani, Nivedita Suresh, and Matthew Peters.Extracting Latent Steering Vectors from Pretrained Language Models.InFindings of the Association for Computational Linguistics: ACL 2022, pages 566–581, Dublin, Ireland, 2022. Association for Computational Linguistics.
- [36]Sheng Liu, Haotian Ye, Lei Xing, and James Zou.In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering, February 2024.arXiv:2311.06668 [cs].
- [37]Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan J. Vazquez, Ulisse Mini, and Monte MacDiarmid.Steering Language Models With Activation Engineering, October 2024.arXiv:2308.10248 [cs].
- [38]Kai Konen, Sophie Jentzsch, Diaoulé Diallo, Peer Schütt, Oliver Bensch, Roxanne El Baff, Dominik Opitz, and Tobias Hecking.Style vectors for steering generative large language models.In Yvette Graham and Matthew Purver, editors,Findings of the Association for Computational Linguistics: EACL 2024, pages 782–802, St. Julian’s, Malta, March 2024. Association for Computational Linguistics.
- [39]Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Turner.Steering Llama 2 via Contrastive Activation Addition.In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15504–15522, Bangkok, Thailand, August 2024. Association for Computational Linguistics.
- [40]Zijian Feng, Hanzhang Zhou, Kezhi Mao, and Zixiao Zhu.FreeCtrl: Constructing Control Centers with Feedforward Layers for Learning-Free Controllable Text Generation.InProceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7627–7640, Bangkok, Thailand, 2024. Association for Computational Linguistics.
- [41]Ziwen Xu, Shuxun Wang, Kewei Xu, Haoming Xu, Mengru Wang, Xinle Deng, Yunzhi Yao, Guozhou Zheng, Huajun Chen, and Ningyu Zhang.EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models, April 2025.arXiv:2504.15133 [cs].
- [42]Yuxin Xiao, Chaoqun Wan, Yonggang Zhang, Wenxiao Wang, Binbin Lin, Xiaofei He, Xu Shen, and Jieping Ye.Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control.November 2024.
- [43]Yu Li, Han Jiang, Chuanyang Gong, and Zhihua Wei.DESTEIN: Navigating Detoxification of Language Models via Universal Steering Pairs and Head-wise Activation Fusion, August 2024.arXiv:2404.10464 [cs].
- [44]Chi Han, Jialiang Xu, Manling Li, Yi Fung, Chenkai Sun, Nan Jiang, Tarek Abdelzaher, and Heng Ji.Word Embeddings Are Steers for Language Models.In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors,Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16410–16430, Bangkok, Thailand, August 2024. Association for Computational Linguistics.
- [45]Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis.Generalization through Memorization: Nearest Neighbor Language Models.September 2019.
- [46]Luiza Pozzobon, Beyza Ermis, Patrick Lewis, and Sara Hooker.Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models.InFindings of the Association for Computational Linguistics: EMNLP 2023, pages 5108–5125, Singapore, 2023. Association for Computational Linguistics.
- [47]Tianyang Xu, Haojie Zheng, Chengze Li, Haoxiang Chen, Yixin Liu, Ruoxi Chen, and Lichao Sun.Noderag: Structuring graph-based rag with heterogeneous nodes, 2025.
- [48]Peiru Yang, Xintian Li, Zhiyang Hu, Jiapeng Wang, Jinhua Yin, Huili Wang, Lizhi He, Shuai Yang, Shangguang Wang, Yongfeng Huang, and Tao Qi.Heterag: A heterogeneous retrieval-augmented generation framework with decoupled knowledge representations, 2025.
- [49]Haoran Luo, Haihong E, Guanting Chen, Yandan Zheng, Xiaobao Wu, Yikai Guo, Qika Lin, Yu Feng, Ze-min Kuang, Meina Song, Yifan Zhu, and Luu Anh Tuan.Hypergraphrag: Retrieval-augmented generation with hypergraph-structured knowledge representation.CoRR, abs/2503.21322, 2025.
- [50]Bernal Jimenez Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su.Hipporag: Neurobiologically inspired long-term memory for large language models.In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors,Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024.
- [51]Bernal Jiménez Gutiérrez, Yiheng Shu, Weijian Qi, Sizhe Zhou, and Yu Su.From RAG to memory: Non-parametric continual learning for large language models.CoRR, abs/2502.14802, 2025.
- [52]Xiang Liang, Simin Niu, Zhiyu Li, Sensen Zhang, Shichao Song, Hanyu Wang, Jiawei Yang, Feiyu Xiong, Bo Tang, and Chenyang Xi.Empowering large language models to set up a knowledge retrieval indexer via self-learning.CoRR, abs/2405.16933, 2024.
- [53]Preston Rasmussen, Pavlo Paliychuk, Travis Beauvais, Jack Ryan, and Daniel Chalef.Zep: A temporal knowledge graph architecture for agent memory.CoRR, abs/2501.13956, 2025.
- [54]Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang.A-MEM: agentic memory for LLM agents.CoRR, abs/2502.12110, 2025.
- [55]Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav.Mem0: Building production-ready ai agents with scalable long-term memory, 2025.
- [56]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.In Jill Burstein, Christy Doran, and Thamar Solorio, editors,Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
- [57]Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback, April 2022.arXiv:2204.05862 [cs].
- [58]Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher.CTRL: A Conditional Transformer Language Model for Controllable Generation, September 2019.arXiv:1909.05858 [cs].
- [59]Tianxiang Chen, Zhentao Tan, Tao Gong, Yue Wu, Qi Chu, Bin Liu, Jieping Ye, and Nenghai Yu.Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection.InFindings of the Association for Computational Linguistics: EMNLP 2024, pages 5991–6002, Miami, Florida, USA, 2024. Association for Computational Linguistics.
- [60]Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.LoRA: Low-Rank Adaptation of Large Language Models, October 2021.arXiv:2106.09685 [cs].
- [61]Weihang Su, Yichen Tang, Qingyao Ai, Junxi Yan, Changyue Wang, Hongning Wang, Ziyi Ye, Yujia Zhou, and Yiqun Liu.Parametric Retrieval Augmented Generation, January 2025.arXiv:2501.15915 [cs].
- [62]Yuqiao Tan, Shizhu He, Huanxuan Liao, Jun Zhao, and Kang Liu.Better wit than wealth: Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement, March 2025.arXiv:2503.23895 [cs].
- [63]Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D. Manning, and Chelsea Finn.Memory-Based Model Editing at Scale.InProceedings of the 39th International Conference on Machine Learning, pages 15817–15831. PMLR, June 2022.ISSN: 2640-3498.
- [64]Qingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu, Zhifang Sui, and Lei Li.Calibrating Factual Knowledge in Pretrained Language Models.InFindings of the Association for Computational Linguistics: EMNLP 2022, pages 5937–5947, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics.
- [65]Xin Cheng, Yankai Lin, Xiuying Chen, Dongyan Zhao, and Rui Yan.Decouple knowledge from paramters for plug-and-play language modeling.InFindings of the Association for Computational Linguistics: ACL 2023, pages 14288–14308, Toronto, Canada, 2023. Association for Computational Linguistics.
- [66]Thomas Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, and Marzyeh Ghassemi.Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors, October 2023.arXiv:2211.11031 [cs].
- [67]Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.Locating and Editing Factual Associations in GPT, January 2023.arXiv:2202.05262 [cs].
- [68]Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau.Mass-Editing Memory in a Transformer, August 2023.arXiv:2210.07229 [cs].
- [69]Junfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Shi Jie, Xiang Wang, Xiangnan He, and Tat-seng Chua.AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models, March 2025.arXiv:2410.02355 [cs].
- [70]Houcheng Jiang, Junfeng Fang, Ningyu Zhang, Guojun Ma, Mingyang Wan, Xiang Wang, Xiangnan He, and Tat-seng Chua.AnyEdit: Edit Any Knowledge Encoded in Language Models, February 2025.arXiv:2502.05628 [cs].
- [71]Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, and Huajun Chen.A Comprehensive Study of Knowledge Editing for Large Language Models, November 2024.arXiv:2401.01286 [cs].
- [72]Qi Li and Xiaowen Chu.Can We Continually Edit Language Models? On the Knowledge Attenuation in Sequential Model Editing.In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors,Findings of the Association for Computational Linguistics: ACL 2024, pages 5438–5455, Bangkok, Thailand, August 2024. Association for Computational Linguistics.
- [73]Daniel Tamayo, Aitor Gonzalez-Agirre, Javier Hernando, and Marta Villegas.Mass-Editing Memory with Attention in Transformers: A cross-lingual exploration of knowledge.InFindings of the Association for Computational Linguistics ACL 2024, pages 5831–5847, 2024.arXiv:2502.02173 [cs].
- [74]Mingyu Jin, Weidi Luo, Sitao Cheng, Xinyi Wang, Wenyue Hua, Ruixiang Tang, William Yang Wang, and Yongfeng Zhang.Disentangling Memory and Reasoning Ability in Large Language Models, November 2024.arXiv:2411.13504 [cs].
- [75]Ali Behrouz, Peilin Zhong, and Vahab Mirrokni.Titans: Learning to memorize at test time.CoRR, abs/2501.00663, 2025.
- [76]Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang.Editing Large Language Models: Problems, Methods, and Opportunities, November 2023.arXiv:2305.13172 [cs].
- [77]Xin Xu, Wei Xu, Ningyu Zhang, and Julian McAuley.BiasEdit: Debiasing Stereotyped Language Models via Model Editing, March 2025.arXiv:2503.08588 [cs].
- [78]Nicola De Cao, Wilker Aziz, and Ivan Titov.Editing Factual Knowledge in Language Models, September 2021.arXiv:2104.08164 [cs].
- [79]Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D. Manning.Fast Model Editing at Scale.October 2021.
- [80]Chenmien Tan, Ge Zhang, and Jie Fu.Massive Editing for Large Language Models via Meta Learning.October 2023.
- [81]Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han.QUEST: Query-Aware Sparsity for Efficient Long-Context LLM Inference.June 2024.
- [82]Chak Leong, Yi Cheng, Jiashuo Wang, Jian Wang, and Wenjie Li.Self-Detoxifying Language Models via Toxification Reversal.InProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4433–4449, Singapore, 2023. Association for Computational Linguistics.
- [83]Tianlong Wang, Xianfeng Jiao, Yinghao Zhu, Zhongzhi Chen, Yifan He, Xu Chu, Junyi Gao, Yasha Wang, and Liantao Ma.Adaptive Activation Steering: A Tuning-Free LLM Truthfulness Improvement Method for Diverse Hallucinations Categories.InProceedings of the ACM on Web Conference 2025, WWW ’25, pages 2562–2578, New York, NY, USA, April 2025. Association for Computing Machinery.
- [84]Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg.Inference-Time Intervention: Eliciting Truthful Answers from a Language Model.Advances in Neural Information Processing Systems, 36:41451–41530, December 2023.
- [85]Pengyu Wang, Dong Zhang, Linyang Li, Chenkun Tan, Xinghao Wang, Mozhi Zhang, Ke Ren, Botian Jiang, and Xipeng Qiu.InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance.pages 10460–10479, November 2024.
- [86]Alessandro Stolfo, Vidhisha Balachandran, Safoora Yousefi, Eric Horvitz, and Besmira Nushi.Improving Instruction-Following in Language Models through Activation Steering, April 2025.arXiv:2410.12877 [cs].
- [87]Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.LLaMA: Open and Efficient Foundation Language Models, February 2023.arXiv:2302.13971 [cs].
- [88]Hugo Touvron, Louis Martin, Kevin Stone, et al.Llama 2: Open Foundation and Fine-Tuned Chat Models, July 2023.arXiv:2307.09288 [cs].
- [89]Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, Jie Zhou, Siming Chen, Tao Gui, Qi Zhang, and Xuanjing Huang.A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models, December 2023.arXiv:2303.10420 [cs].
- [90]Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.Lost in the Middle: How Language Models Use Long Contexts.Transactions of the Association for Computational Linguistics, 12:157–173, 2024.Place: Cambridge, MA Publisher: MIT Press.
- [91]Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Baobao Chang, Xu Sun, Lei Li, and Zhifang Sui.A Survey on In-context Learning.In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors,Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1107–1128, Miami, Florida, USA, November 2024. Association for Computational Linguistics.
- [92]Stephen E. Robertson and Hugo Zaragoza.The probabilistic relevance framework: BM25 and beyond.Found. Trends Inf. Retr., 3(4):333–389, 2009.
- [93]Nils Reimers and Iryna Gurevych.Sentence-bert: Sentence embeddings using siamese bert-networks.InProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3980–3990, 2019.
- [94]Langchaln.Ensemble retriever.https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/ensemble.
- [95]Jiale Wei, Xiang Ying, Tao Gao, Fangyi Bao, Felix Tao, and Jingbo Shang.Ai-native memory 2.0: Second me, 2025.
- [96]Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang.Autogen: Enabling next-gen LLM applications via multi-agent conversation framework.CoRR, abs/2308.08155, 2023.
- [97]Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav.Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory, April 2025.arXiv:2504.19413 [cs].
- [98]Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez.MemGPT: Towards LLMs as Operating Systems, February 2024.arXiv:2310.08560 [cs].
- [99]Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy.Lima: Less is more for alignment, 2023.