{"file": "arxiv-2507.03724v3.md", "path": "docs/processed/arxiv-2507.03724v3/arxiv-2507.03724v3.md", "description": "MemOS \\ul1]MemTensor (Shanghai) Technology Co., Ltd. 2]Shanghai Jiao Tong University 3]Institute for Advanced Algorithms Research, Shanghai 4]Tongji University 5]Zhejiang University 6]University of Science and Technology of China 7]Peking Universi...", "priority": 4, "bytes": 281478, "type": "research"}
{"file": "arxiv-2507.03724v3_1introduction.md", "path": "docs/processed/arxiv-2507.03724v3/arxiv-2507.03724v3_1introduction.md", "description": "Introduction to With the advent of the Transformer architecture and the maturation of self-supervised pretraining, Large Language Models (LLMs) have become the cornerstone of modern NLP. Trained on large-scale corpora, LLMs exhibit near-human perf...", "priority": 4, "bytes": 17629, "type": "research"}
{"file": "arxiv-2507.03724v3_2memory-in-large-language-models.md", "path": "docs/processed/arxiv-2507.03724v3/arxiv-2507.03724v3_2memory-in-large-language-models.md", "description": "Research in memory capabilities in large language models has generally progressed through four key stages:(1) The stage of definition and exploration, which focuses on categorizing and analyzing LLM memory systems from multiple perspectives, while...", "priority": 4, "bytes": 21823, "type": "research"}
{"file": "arxiv-2507.03724v3_3memosdesign-philosophy.md", "path": "docs/processed/arxiv-2507.03724v3/arxiv-2507.03724v3_3memosdesign-philosophy.md", "description": "As AGI advances toward increasingly complex systems involving multiple tasks, roles, and modalities, LLMs must go beyond merely “understanding the world”—they must also “accumulate experience,” “retain memory,” and “continuously evolve.” However, ...", "priority": 4, "bytes": 9327, "type": "research"}
{"file": "arxiv-2507.03724v3_4memory-modeling-inmemos.md", "path": "docs/processed/arxiv-2507.03724v3/arxiv-2507.03724v3_4memory-modeling-inmemos.md", "description": "The concept of hierarchical memory was originally introduced in our prior work Memory3[1], which proposed a distinction between explicit and implicit memory paths in LLMs and investigated their interaction mechanisms.", "priority": 4, "bytes": 11665, "type": "research"}
{"file": "arxiv-2507.03724v3_5architecture-ofmemos.md", "path": "docs/processed/arxiv-2507.03724v3/arxiv-2507.03724v3_5architecture-ofmemos.md", "description": "Architecture documentation of MemOSadopts a modular three-layer architecture to support efficient invocation, dynamic scheduling, and compliant governance of complex memory tasks (see Figure7). It consists of the Interface Layer, Operation Layer, ...", "priority": 4, "bytes": 26132, "type": "research"}
{"file": "arxiv-2507.03724v3_6evaluation.md", "path": "docs/processed/arxiv-2507.03724v3/arxiv-2507.03724v3_6evaluation.md", "description": "To systematically evaluate the capabilities ofMemOS, we conduct both holistic and component-level experiments. We begin by benchmarking the full system on the LOCOMO benchmark suite to assess its performance in memory-intensive reasoning tasks, co...", "priority": 4, "bytes": 19848, "type": "research"}
{"file": "arxiv-2507.03724v3_7memosfor-architecture-innovation-and-applications.md", "path": "docs/processed/arxiv-2507.03724v3/arxiv-2507.03724v3_7memosfor-architecture-innovation-and-applications.md", "description": "Architecture documentation of  7MemOSfor Architecture Innovation and Applications.", "priority": 4, "bytes": 9895, "type": "research"}
{"file": "arxiv-2507.03724v3_8conclusion.md", "path": "docs/processed/arxiv-2507.03724v3/arxiv-2507.03724v3_8conclusion.md", "description": "In this work, we introduce a memory operating system designed for Large Language Models, aimed at collaboratively building foundational memory infrastructure for next-generation LLM applications.", "priority": 4, "bytes": 3573, "type": "research"}
{"file": "arxiv-2507.03724v3_a-memory-os-for-ai-system.md", "path": "docs/processed/arxiv-2507.03724v3/arxiv-2507.03724v3_a-memory-os-for-ai-system.md", "description": "MemOS Large Language Models (LLMs) have become an essential infrastructure for Artificial General Intelligence (AGI), yet their lack of well-defined memory management systems hinders the development of long-context reasoning, continual personaliza...", "priority": 4, "bytes": 2293, "type": "research"}
{"file": "arxiv-2507.03724v3_references.md", "path": "docs/processed/arxiv-2507.03724v3/arxiv-2507.03724v3_references.md", "description": "- [1]Hongkang Yang, Zehao Lin, Wenjin Wang, Hao Wu, Zhiyu Li, Bo Tang, Wenqiang Wei, Jinbo Wang, Zeyun Tang, Shichao Song, Chenyang Xi, Yu Yu, Kai Chen, Feiyu Xiong, Linpeng Tang, and Weinan E.Memory3: Language modeling with explicit memory.Journa...", "priority": 4, "bytes": 24331, "type": "api-reference"}
