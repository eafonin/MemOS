# Example docker-compose.yml configuration for BGE-Large embeddings
#
# This file shows the required changes to docker-compose.yml to use
# BAAI/bge-large-en-v1.5 with 512-token support.
#
# USAGE:
#   Copy the 'tei' service configuration below to your docker-compose.yml
#   OR manually update the model-id and add --auto-truncate flag

services:
  # Text Embeddings Inference Service (BGE-Large with 512-token support)
  tei:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.8.1
    container_name: memos-tei
    ports:
      - "8081:80"
    volumes:
      - ./data/hf-cache:/data
    environment:
      - MODEL_ID=BAAI/bge-large-en-v1.5
      - REVISION=main
      - HF_HUB_ENABLE_HF_TRANSFER=1
    command: >
      --model-id BAAI/bge-large-en-v1.5
      --port 80
      --auto-truncate
    networks:
      - memos_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

# NOTES:
# - Model download size: ~1.34 GB (first run only)
# - First startup will take 5-10 minutes to download model
# - Model is cached in ./data/hf-cache for future use
# - --auto-truncate flag provides safety net for edge cases (single sentences >512 tokens)
# - Verify with: curl http://localhost:8081/info | jq '.max_input_length'
#   Should return: 512
